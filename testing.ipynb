{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "from safetensors.torch import laod_file\n",
    "\n",
    "# Provide the path to the folder containing the model config and weights,\n",
    "# not directly to a .safetensors checkpoint file.\n",
    "model_dir = 'runwayml/stable-diffusion-v1-5'\n",
    "\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    model_dir, \n",
    "    torch_dtype=torch.float16, \n",
    "    local_files_only=True\n",
    ")\n",
    "pipeline.to(\"cuda\")\n",
    "\n",
    "# Disable NSFW filter\n",
    "\n",
    "pipeline.safety_checker = None\n",
    "pipeline.requires_safety_checking = False\n",
    "\n",
    "pipeline.load_lora_weights(r\"C:\\Users\\013\\Desktop\\ml_image_server\\checkpoints\", weight_name=\"saekdam-10.safetensors\")\n",
    "pipeline.load_lora_weights(r\"C:\\Users\\013\\Desktop\\ml_image_server\\checkpoints\", weight_name=\"gru-10.safetensors\")\n",
    "\n",
    "\n",
    "# Load LoRA weights separately (assuming this file is valid for the modifications)\n",
    "# lora_weights = load_file(r\"C:\\Users\\013\\Desktop\\ml_image_server\\flat.safetensors\")\n",
    "# pipe.unet.load_state_dict(lora_weights, strict=False)  # Apply LoRA weights to U-Net\n",
    "\n",
    "prompt = \"smiling groot, smiling saesoon, holding hands, flat design, middle of the night\"\n",
    "\n",
    "image = pipeline(prompt).images[0]\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install PyTorch using:\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "import torch\n",
    "\n",
    "from models.vision.StableDiffusionControlNet import StableDiffusionControlNetModel\n",
    "from PIL import Image\n",
    "\n",
    "cuda = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "\n",
    "stable_diffuser_model = load_diffuser_controlnet_model_checkpoint(cuda=cuda)\n",
    "\n",
    "control_image = Image.open(r\"C:\\Users\\013\\Desktop\\ml_image_server\\wall_mask.png\")\n",
    "wall_image = Image.open(r\"C:\\Users\\013\\Desktop\\ml_image_server\\original_image.png\")\n",
    "\n",
    "text_prompt = \"ocean, saekdam style, flat design, wall painting\"\n",
    "\n",
    "stable_diffuser_model.generate_painting(control_image, text_prompt, wall_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cuda = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "# Clear CUDA cache before loading model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "# Reduce image dimensions to decrease memory requirements\n",
    "prompt = \"A cat holding a sign that says hello world\" \n",
    "image = pipe(\n",
    "    prompt,\n",
    "    height=512,  # Reduced from 1024\n",
    "    width=512,   # Reduced from 1024\n",
    "    guidance_scale=3.5,\n",
    "    num_inference_steps=50,\n",
    "    max_sequence_length=512,\n",
    "    generator=torch.Generator(device=\"cuda\").manual_seed(0)\n",
    ").images[0]\n",
    "image.save(\"flux-dev.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"CUDA is not available. Please use a GPU-enabled machine.\")\n",
    "\n",
    "# Add version checking for PyTorch and CUDA\n",
    "def check_cuda_compatibility():\n",
    "    cuda_version = torch.version.cuda\n",
    "    torch_version = torch.__version__\n",
    "    print(f\"PyTorch version: {torch_version}\")\n",
    "    print(f\"CUDA version: {cuda_version}\")\n",
    "    \n",
    "    try:\n",
    "        import xformers\n",
    "        print(f\"xformers version: {xformers.__version__}\")\n",
    "    except ImportError:\n",
    "        print(\"Warning: xformers not installed. Some optimizations will not be available.\")\n",
    "        print(\"To install xformers, run: pip install -U xformers --index-url https://download.pytorch.org/whl/cu121\")\n",
    "\n",
    "check_cuda_compatibility()\n",
    "cuda = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "cuda = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {cuda}\")\n",
    "\n",
    "# 예외 발생 대신 경고 메시지 출력\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: CUDA is not available. Running on CPU will be slower.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "base_model = \"black-forest-labs/FLUX.1-dev\"\n",
    "pipe = DiffusionPipeline.from_pretrained(base_model, torch_dtype=torch.bfloat16)\n",
    "\n",
    "lora_repo = \"strangerzonehf/Flux-Super-Paint-LoRA\"\n",
    "trigger_word = \"Super Paint\"  \n",
    "pipe.load_lora_weights(lora_repo)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "pipe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "# Load and view the safetensors file\n",
    "tensor_path = \"C:\\\\Users\\\\013\\\\Desktop\\\\ml_image_server\\\\checkpoints\\\\Super-Paint.safetensors\"\n",
    "\n",
    "with safe_open(tensor_path, framework=\"pt\") as f:\n",
    "    # Get metadata if available\n",
    "    metadata = f.metadata()\n",
    "    if metadata:\n",
    "        print(\"Metadata:\", metadata)\n",
    "    \n",
    "    # Print tensor names and shapes to check available target modules\n",
    "    target_modules = {'attn.proj', 'lin', 'mlp.0', 'linear1', 'mlp.2', 'qkv', 'linear2'}\n",
    "    found_modules = set()\n",
    "    \n",
    "    for tensor_name in f.keys():\n",
    "        tensor = f.get_tensor(tensor_name)\n",
    "        print(f\"Tensor: {tensor_name}\")\n",
    "        print(f\"Shape: {tensor.shape}\")\n",
    "        print(f\"Dtype: {tensor.dtype}\")\n",
    "        print(\"---\")\n",
    "        \n",
    "        # Check if tensor name contains any target module names\n",
    "        for target in target_modules:\n",
    "            if target in tensor_name:\n",
    "                found_modules.add(target)\n",
    "                \n",
    "    print(\"\\nTarget modules found:\", found_modules)\n",
    "    print(\"Missing target modules:\", target_modules - found_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "# Load and view the safetensors file\n",
    "tensor_path = r\"C:\\Users\\013\\Desktop\\ml_image_server\\checkpoints\\urban-10.safetensors\"\n",
    "\n",
    "with safe_open(tensor_path, framework=\"pt\") as f:\n",
    "    # Get metadata if available\n",
    "    metadata = f.metadata()\n",
    "    if metadata:\n",
    "        print(\"Metadata:\", metadata)\n",
    "    \n",
    "    # Print tensor names and shapes\n",
    "    for tensor_name in f.keys():\n",
    "        tensor = f.get_tensor(tensor_name)\n",
    "        print(f\"Tensor: {tensor_name}\")\n",
    "        print(f\"Shape: {tensor.shape}\")\n",
    "        print(f\"Dtype: {tensor.dtype}\")\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "# Load and view the safetensors file\n",
    "tensor_path = r\"C:\\Users\\013\\Desktop\\ml_image_server\\checkpoints\\urban-10.safetensors\"\n",
    "\n",
    "with safe_open(tensor_path, framework=\"pt\") as f:\n",
    "    # Get metadata if available\n",
    "    metadata = f.metadata()\n",
    "    if metadata:\n",
    "        print(\"Metadata:\", metadata)\n",
    "    \n",
    "    # Print tensor names and shapes to check available target modules\n",
    "    target_modules = {'attn.proj', 'lin', 'mlp.0', 'linear1', 'mlp.2', 'qkv', 'linear2'}\n",
    "    found_modules = set()\n",
    "    \n",
    "    for tensor_name in f.keys():\n",
    "        tensor = f.get_tensor(tensor_name)\n",
    "        print(f\"Tensor: {tensor_name}\")\n",
    "        print(f\"Shape: {tensor.shape}\")\n",
    "        print(f\"Dtype: {tensor.dtype}\")\n",
    "        print(\"---\")\n",
    "        \n",
    "        # Check if tensor name contains any target module names\n",
    "        for target in target_modules:\n",
    "            if target in tensor_name:\n",
    "                found_modules.add(target)\n",
    "                \n",
    "    print(\"\\nTarget modules found:\", found_modules)\n",
    "    print(\"Missing target modules:\", target_modules - found_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "import gc\n",
    "\n",
    "# Enable memory efficient attention\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Free up CUDA memory more aggressively\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Set environment variable for CUDA error debugging\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "base_model = \"black-forest-labs/FLUX.1-dev\"\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    base_model,\n",
    "    torch_dtype=torch.float16, # Use float16 instead of bfloat16 for less memory\n",
    "    use_safetensors=True,\n",
    "    # Load model in low memory mode\n",
    "    low_cpu_mem_usage=True,\n",
    "    # Use attention optimization\n",
    "    use_memory_efficient_attention=True,\n",
    "    # Load model components sequentially to reduce peak memory\n",
    "    sequential_cpu_offload=True\n",
    ")\n",
    "\n",
    "# Clear memory again before loading LoRA\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "lora_repo = \"strangerzonehf/Flux-Super-Paint-LoRA\"\n",
    "trigger_word = \"Super Paint\"\n",
    "pipe.load_lora_weights(lora_repo)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "# Move model to device gradually to avoid OOM\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.08s/it]\n",
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:24<00:50, 10.06s/it]"
     ]
    }
   ],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "import torch\n",
    "\n",
    "pipeline = AutoPipelineForText2Image.from_pretrained('black-forest-labs/FLUX.1-dev', torch_dtype=torch.float16).to('cuda')\n",
    "pipeline.load_lora_weights(r'C:\\Users\\013\\Desktop\\ml_image_server\\checkpoints', weight_name='Super-Paint.safetensors')\n",
    "image = pipeline('hi').images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.96it/s]it/s]\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  5.12it/s]\n",
      " 90%|█████████ | 45/50 [1:28:48<09:52, 118.42s/it]   \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m pipe\u001b[38;5;241m.\u001b[39menable_model_cpu_offload() \u001b[38;5;66;03m#save some VRAM by offloading the model to CPU. Remove this if you have enough GPU power\u001b[39;00m\n\u001b[0;32m     10\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA cat holding a sign that says hello world\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_sequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     20\u001b[0m image\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflux-dev.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\013\\Desktop\\ml_image_server\\dev\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\013\\Desktop\\ml_image_server\\dev\\Lib\\site-packages\\diffusers\\pipelines\\flux\\pipeline_flux.py:889\u001b[0m, in \u001b[0;36mFluxPipeline.__call__\u001b[1;34m(self, prompt, prompt_2, negative_prompt, negative_prompt_2, true_cfg_scale, height, width, num_inference_steps, sigmas, guidance_scale, num_images_per_prompt, generator, latents, prompt_embeds, pooled_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, negative_ip_adapter_image, negative_ip_adapter_image_embeds, negative_prompt_embeds, negative_pooled_prompt_embeds, output_type, return_dict, joint_attention_kwargs, callback_on_step_end, callback_on_step_end_tensor_inputs, max_sequence_length)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;66;03m# broadcast to batch dimension in a way that's compatible with ONNX/Core ML\u001b[39;00m\n\u001b[0;32m    887\u001b[0m timestep \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mexpand(latents\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(latents\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m--> 889\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpooled_projections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpooled_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtxt_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatent_image_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_true_cfg:\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m negative_image_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\013\\Desktop\\ml_image_server\\dev\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\013\\Desktop\\ml_image_server\\dev\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\013\\Desktop\\ml_image_server\\dev\\Lib\\site-packages\\accelerate\\hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\013\\Desktop\\ml_image_server\\dev\\Lib\\site-packages\\diffusers\\models\\transformers\\transformer_flux.py:522\u001b[0m, in \u001b[0;36mFluxTransformer2DModel.forward\u001b[1;34m(self, hidden_states, encoder_hidden_states, pooled_projections, timestep, img_ids, txt_ids, guidance, joint_attention_kwargs, controlnet_block_samples, controlnet_single_block_samples, return_dict, controlnet_blocks_repeat)\u001b[0m\n\u001b[0;32m    512\u001b[0m     encoder_hidden_states, hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    513\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    514\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mckpt_kwargs,\n\u001b[0;32m    519\u001b[0m     )\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 522\u001b[0m     encoder_hidden_states, hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;66;03m# controlnet residual\u001b[39;00m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m controlnet_block_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\013\\Desktop\\ml_image_server\\dev\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\013\\Desktop\\ml_image_server\\dev\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\013\\Desktop\\ml_image_server\\dev\\Lib\\site-packages\\diffusers\\models\\transformers\\transformer_flux.py:202\u001b[0m, in \u001b[0;36mFluxTransformerBlock.forward\u001b[1;34m(self, hidden_states, encoder_hidden_states, temb, image_rotary_emb, joint_attention_kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m ff_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff(norm_hidden_states)\n\u001b[0;32m    200\u001b[0m ff_output \u001b[38;5;241m=\u001b[39m gate_mlp\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m ff_output\n\u001b[1;32m--> 202\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mff_output\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(attention_outputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    204\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m ip_attn_output\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\n",
    "pipe.enable_model_cpu_offload() #save some VRAM by offloading the model to CPU. Remove this if you have enough GPU power\n",
    "\n",
    "prompt = \"A cat holding a sign that says hello world\"\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    height=512,\n",
    "    width=512,\n",
    "    guidance_scale=3.5,\n",
    "    num_inference_steps=50,\n",
    "    max_sequence_length=512,\n",
    "    generator=torch.Generator(device=device).manual_seed(0)\n",
    ").images[0]\n",
    "image.save(\"flux-dev.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
